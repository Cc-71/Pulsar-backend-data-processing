#include "cuda_fft.h"
#include <iostream>
#include <cuda_runtime.h>
#include <cufft.h>
#include <vector>
#include <complex>
#include <thread>
#include <mutex>
#include <atomic>
#include <memory>

#define MAX_STREAMS_PER_GPU 4
#define MAX_GPUS 8

// GPUè®¾å¤‡ç®¡ç†ç»“æ„
struct GPUDevice {
    int device_id;
    bool available;
    std::mutex device_mutex;
    cudaStream_t streams[MAX_STREAMS_PER_GPU];
    cufftHandle plans[MAX_STREAMS_PER_GPU];

    // è®¾å¤‡å†…å­˜æ±  - ä¿®å¤ï¼šä¸ºæ‰¹é‡FFTåˆ†é…è¶³å¤Ÿå†…å­˜
    int8_t* d_int8_pool[MAX_STREAMS_PER_GPU];
    float* d_float_pool[MAX_STREAMS_PER_GPU];
    cufftComplex* d_fft_pool[MAX_STREAMS_PER_GPU];
    float* d_power_pool[MAX_STREAMS_PER_GPU];

    std::atomic<bool> stream_busy[MAX_STREAMS_PER_GPU];
    std::atomic<uint64_t> processed_count{0};
    std::atomic<uint64_t> error_count{0};

    GPUDevice() : device_id(-1), available(false) {
        for (int i = 0; i < MAX_STREAMS_PER_GPU; ++i) {
            stream_busy[i] = false;
            d_int8_pool[i] = nullptr;
            d_float_pool[i] = nullptr;
            d_fft_pool[i] = nullptr;
            d_power_pool[i] = nullptr;
        }
    }
};

// å…¨å±€GPUç®¡ç†å™¨
class MultiGPUManager {
private:
    std::vector<std::unique_ptr<GPUDevice>> devices_;
    std::atomic<int> next_device_{0};
    int num_available_gpus_{0};
    std::mutex init_mutex_;
    bool initialized_{false};

public:
    MultiGPUManager() = default;

    ~MultiGPUManager() {
        cleanup();
    }

    bool initialize() {
        std::lock_guard<std::mutex> lock(init_mutex_);
        if (initialized_) return true;

        int device_count = 0;
        cudaError_t error = cudaGetDeviceCount(&device_count);
        if (error != cudaSuccess) {
            std::cerr << "âŒ æ— æ³•è·å–GPUæ•°é‡: " << cudaGetErrorString(error) << std::endl;
            return false;
        }

        std::cout << "ğŸ” æ£€æµ‹åˆ° " << device_count << " ä¸ªGPUè®¾å¤‡" << std::endl;

        // åˆå§‹åŒ–æ¯ä¸ªå¯ç”¨GPU
        for (int i = 0; i < device_count && i < MAX_GPUS; ++i) {
            auto device = std::make_unique<GPUDevice>();
            device->device_id = i;

            if (initializeGPUDevice(*device)) {
                devices_.push_back(std::move(device));
                num_available_gpus_++;
                std::cout << "âœ… GPU " << i << " åˆå§‹åŒ–æˆåŠŸ" << std::endl;
            } else {
                std::cerr << "âŒ GPU " << i << " åˆå§‹åŒ–å¤±è´¥" << std::endl;
            }
        }

        if (num_available_gpus_ == 0) {
            std::cerr << "âŒ æ²¡æœ‰å¯ç”¨çš„GPUè®¾å¤‡" << std::endl;
            return false;
        }

        std::cout << "ğŸš€ å¤šGPUç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨GPUæ•°é‡: " << num_available_gpus_ << std::endl;
        initialized_ = true;
        return true;
    }

    int getAvailableGPUCount() const {
        return num_available_gpus_;
    }

    // è·å–æœ€ä½³GPUè®¾å¤‡ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
    GPUDevice* getBestGPU() {
        if (devices_.empty()) return nullptr;

        // å¯»æ‰¾è´Ÿè½½æœ€è½»çš„GPU
        GPUDevice* best_device = nullptr;
        int min_busy_streams = MAX_STREAMS_PER_GPU + 1;

        for (auto& device : devices_) {
            if (!device->available) continue;

            int busy_count = 0;
            for (int i = 0; i < MAX_STREAMS_PER_GPU; ++i) {
                if (device->stream_busy[i].load()) {
                    busy_count++;
                }
            }

            if (busy_count < min_busy_streams) {
                min_busy_streams = busy_count;
                best_device = device.get();
            }
        }

        return best_device;
    }

    // è·å–æŒ‡å®šGPUè®¾å¤‡
    GPUDevice* getGPU(int gpu_id) {
        for (auto& device : devices_) {
            if (device->device_id == gpu_id) {
                return device.get();
            }
        }
        return nullptr;
    }

    void printStats() {
        std::cout << "ğŸ“Š å¤šGPUå¤„ç†ç»Ÿè®¡:" << std::endl;
        for (const auto& device : devices_) {
            int busy_streams = 0;
            for (int i = 0; i < MAX_STREAMS_PER_GPU; ++i) {
                if (device->stream_busy[i].load()) busy_streams++;
            }
            std::cout << "   GPU " << device->device_id
                      << ": å¤„ç†=" << device->processed_count.load()
                      << ", é”™è¯¯=" << device->error_count.load()
                      << ", å¿™ç¢Œæµ=" << busy_streams << "/" << MAX_STREAMS_PER_GPU << std::endl;
        }
    }

private:
    bool initializeGPUDevice(GPUDevice& device) {
        cudaError_t error = cudaSetDevice(device.device_id);
        if (error != cudaSuccess) {
            std::cerr << "æ— æ³•è®¾ç½®GPUè®¾å¤‡ " << device.device_id << ": " << cudaGetErrorString(error) << std::endl;
            return false;
        }

        // æ£€æŸ¥GPUå±æ€§
        cudaDeviceProp prop;
        cudaGetDeviceProperties(&prop, device.device_id);
        std::cout << "ğŸ”§ GPU " << device.device_id << ": " << prop.name
                  << " (æ˜¾å­˜: " << prop.totalGlobalMem / (1024*1024) << "MB)" << std::endl;

        // ä¿®å¤ï¼šä½¿ç”¨æ‰¹é‡FFTçš„æ­£ç¡®å¤§å°
        const size_t N = 64 * 1024;
        const int batch = 3;  // æ‰¹é‡å¤§å°

        // ä¸ºæ¯ä¸ªGPUåˆ›å»ºæµå’Œå†…å­˜æ± 
        for (int i = 0; i < MAX_STREAMS_PER_GPU; ++i) {
            // åˆ›å»ºCUDAæµ
            error = cudaStreamCreate(&device.streams[i]);
            if (error != cudaSuccess) {
                std::cerr << "åˆ›å»ºCUDAæµå¤±è´¥: " << cudaGetErrorString(error) << std::endl;
                return false;
            }

            // ä¿®å¤ï¼šåˆ›å»ºæ‰¹é‡FFTè®¡åˆ’
            int n[1] = { static_cast<int>(N) };
            cufftResult fft_result = cufftPlanMany(&device.plans[i], 1, n,
                                                  nullptr, 1, N,
                                                  nullptr, 1, N/2 + 1,
                                                  CUFFT_R2C, batch);  // æ‰¹é‡å¤§å°
            if (fft_result != CUFFT_SUCCESS) {
                std::cerr << "åˆ›å»ºCUFFTè®¡åˆ’å¤±è´¥ï¼Œé”™è¯¯ä»£ç : " << fft_result << std::endl;
                return false;
            }

            cufftSetStream(device.plans[i], device.streams[i]);

            // é¢„åˆ†é…è®¾å¤‡å†…å­˜æ± ï¼ˆæ‰¹é‡å¤§å°ï¼‰
            if (!allocateDeviceMemory(device, i, N, batch)) {
                std::cerr << "GPU " << device.device_id << " æµ " << i << " å†…å­˜åˆ†é…å¤±è´¥" << std::endl;
                return false;
            }
        }

        device.available = true;
        return true;
    }

    bool allocateDeviceMemory(GPUDevice& device, int stream_idx, size_t N, int batch) {
        cudaError_t error;

        std::cout << "ğŸ”§ ä¸ºGPU " << device.device_id << " æµ " << stream_idx
                  << " åˆ†é…å†…å­˜: " << (N * batch * sizeof(int8_t) / 1024 / 1024) << "MB" << std::endl;

        // ä¿®å¤ï¼šæŒ‰æ‰¹é‡å¤§å°åˆ†é…å†…å­˜
        error = cudaMalloc(&device.d_int8_pool[stream_idx], N * batch * sizeof(int8_t));
        if (error != cudaSuccess) {
            std::cerr << "åˆ†é…int8å†…å­˜å¤±è´¥: " << cudaGetErrorString(error) << std::endl;
            return false;
        }

        error = cudaMalloc(&device.d_float_pool[stream_idx], N * batch * sizeof(float));
        if (error != cudaSuccess) {
            std::cerr << "åˆ†é…floatå†…å­˜å¤±è´¥: " << cudaGetErrorString(error) << std::endl;
            return false;
        }

        error = cudaMalloc(&device.d_fft_pool[stream_idx], (N/2 + 1) * batch * sizeof(cufftComplex));
        if (error != cudaSuccess) {
            std::cerr << "åˆ†é…FFTå†…å­˜å¤±è´¥: " << cudaGetErrorString(error) << std::endl;
            return false;
        }

        error = cudaMalloc(&device.d_power_pool[stream_idx], (N/2 + 1) * batch * sizeof(float));
        if (error != cudaSuccess) {
            std::cerr << "åˆ†é…åŠŸç‡è°±å†…å­˜å¤±è´¥: " << cudaGetErrorString(error) << std::endl;
            return false;
        }

        return true;
    }

    void cleanup() {
        for (auto& device : devices_) {
            if (device->available) {
                cudaSetDevice(device->device_id);

                for (int i = 0; i < MAX_STREAMS_PER_GPU; ++i) {
                    if (device->d_int8_pool[i]) cudaFree(device->d_int8_pool[i]);
                    if (device->d_float_pool[i]) cudaFree(device->d_float_pool[i]);
                    if (device->d_fft_pool[i]) cudaFree(device->d_fft_pool[i]);
                    if (device->d_power_pool[i]) cudaFree(device->d_power_pool[i]);

                    cufftDestroy(device->plans[i]);
                    cudaStreamDestroy(device->streams[i]);
                }
            }
        }
        devices_.clear();
    }
};

// å…¨å±€å¤šGPUç®¡ç†å™¨å®ä¾‹
static MultiGPUManager g_gpu_manager;

// CUDAå†…æ ¸å®šä¹‰ï¼ˆä¸åŸæ¥ç›¸åŒï¼‰
__global__ void ConvertInt8ToFloat(int8_t* d_int8, float* d_float, size_t N) {
    const size_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        d_float[i] = static_cast<float>(d_int8[i]);
    }
}

__global__ void AbsFFTResults(cufftComplex* fft_results, float* abs_results, size_t N) {
    const size_t i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        abs_results[i] = fft_results[i].x * fft_results[i].x + fft_results[i].y * fft_results[i].y;
    }
}

__global__ void SumFFTResultsInPlace(float* fft_abs_results, size_t fft_size, int batch_count) {
    const size_t k = blockIdx.x * blockDim.x + threadIdx.x;
    if (k >= fft_size) return;

    float sum = fft_abs_results[k];
    for (int b = 1; b < batch_count; ++b) {
        sum += fft_abs_results[k + b * fft_size];
    }
    fft_abs_results[k] = sum;
}

// ä¿®å¤åçš„å¤šGPUå¼‚æ­¥FFTå¤„ç†å‡½æ•°
bool cuda_fft_process_batch_async_multigpu(const std::vector<int8_t>& chunk_data,
                                           std::vector<std::complex<float>>& fft_result) {
    const size_t N = 64 * 1024;
    const int batch = 3;

    if (chunk_data.size() != N * batch) {
        std::cerr << "âŒ è¾“å…¥æ•°æ®å¤§å°é”™è¯¯: æœŸæœ› " << (N * batch) << ", å®é™… " << chunk_data.size() << std::endl;
        return false;
    }

    // è·å–æœ€ä½³GPUè®¾å¤‡
    GPUDevice* device = g_gpu_manager.getBestGPU();
    if (!device || !device->available) {
        std::cerr << "âŒ æ²¡æœ‰å¯ç”¨çš„GPUè®¾å¤‡" << std::endl;
        return false;
    }

    // åŸå­æ“ä½œå¯»æ‰¾ç©ºé—²çš„æµ
    int stream_idx = -1;
    for (int i = 0; i < MAX_STREAMS_PER_GPU; ++i) {
        bool expected = false;
        if (device->stream_busy[i].compare_exchange_weak(expected, true)) {
            stream_idx = i;
            break;
        }
    }

    if (stream_idx == -1) {
        // æ²¡æœ‰ç©ºé—²æµï¼Œç­‰å¾…ä¸€ä¸ªçŸ­æ—¶é—´åé‡è¯•
        std::this_thread::sleep_for(std::chrono::microseconds(50));
        return false;
    }

    // è®¾ç½®GPUè®¾å¤‡ä¸Šä¸‹æ–‡
    cudaError_t error = cudaSetDevice(device->device_id);
    if (error != cudaSuccess) {
        std::cerr << "âŒ è®¾ç½®GPUè®¾å¤‡å¤±è´¥: " << cudaGetErrorString(error) << std::endl;
        device->stream_busy[stream_idx] = false;
        device->error_count++;
        return false;
    }

    bool success = true;
    try {
        // ä½¿ç”¨é¢„åˆ†é…çš„å†…å­˜æ± 
        int8_t* d_int8 = device->d_int8_pool[stream_idx];
        float* d_float = device->d_float_pool[stream_idx];
        cufftComplex* d_fft_output = device->d_fft_pool[stream_idx];
        float* d_power = device->d_power_pool[stream_idx];
        cudaStream_t stream = device->streams[stream_idx];
        cufftHandle plan = device->plans[stream_idx];

        // å¼‚æ­¥å†…å­˜ä¼ è¾“ï¼ˆç°åœ¨å¤§å°åŒ¹é…äº†ï¼‰
        error = cudaMemcpyAsync(d_int8, chunk_data.data(), N * batch * sizeof(int8_t),
                               cudaMemcpyHostToDevice, stream);
        if (error != cudaSuccess) {
            throw std::runtime_error("å†…å­˜ä¼ è¾“å¤±è´¥: " + std::string(cudaGetErrorString(error)));
        }

        // ç±»å‹è½¬æ¢
        const int blockSize = 256;
        const int gridSize = (N * batch + blockSize - 1) / blockSize;
        ConvertInt8ToFloat<<<gridSize, blockSize, 0, stream>>>(d_int8, d_float, N * batch);

        // æ£€æŸ¥kernelæ‰§è¡Œé”™è¯¯
        error = cudaGetLastError();
        if (error != cudaSuccess) {
            throw std::runtime_error("ConvertInt8ToFloat kernelå¤±è´¥: " + std::string(cudaGetErrorString(error)));
        }

        // æ‰§è¡Œæ‰¹é‡FFT
        cufftResult fft_result_code = cufftExecR2C(plan, d_float, d_fft_output);
        if (fft_result_code != CUFFT_SUCCESS) {
            throw std::runtime_error("FFTæ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ä»£ç : " + std::to_string(fft_result_code));
        }

        // è®¡ç®—åŠŸç‡è°±
        const size_t fft_size = N/2 + 1;
        const int absGridSize = (fft_size * batch + blockSize - 1) / blockSize;
        AbsFFTResults<<<absGridSize, blockSize, 0, stream>>>(
            d_fft_output, d_power, fft_size * batch);

        // æ£€æŸ¥kernelæ‰§è¡Œé”™è¯¯
        error = cudaGetLastError();
        if (error != cudaSuccess) {
            throw std::runtime_error("AbsFFTResults kernelå¤±è´¥: " + std::string(cudaGetErrorString(error)));
        }

        // åŸåœ°æ±‚å’Œ
        const int sumGridSize = (fft_size + blockSize - 1) / blockSize;
        SumFFTResultsInPlace<<<sumGridSize, blockSize, 0, stream>>>(
            d_power, fft_size, batch);

        // æ£€æŸ¥kernelæ‰§è¡Œé”™è¯¯
        error = cudaGetLastError();
        if (error != cudaSuccess) {
            throw std::runtime_error("SumFFTResultsInPlace kernelå¤±è´¥: " + std::string(cudaGetErrorString(error)));
        }

        // å¼‚æ­¥ä¼ è¾“ç»“æœ
        std::vector<float> power_result(fft_size);
        error = cudaMemcpyAsync(power_result.data(), d_power, fft_size * sizeof(float),
                               cudaMemcpyDeviceToHost, stream);
        if (error != cudaSuccess) {
            throw std::runtime_error("ç»“æœä¼ è¾“å¤±è´¥: " + std::string(cudaGetErrorString(error)));
        }

        // ç­‰å¾…æµå®Œæˆ
        error = cudaStreamSynchronize(stream);
        if (error != cudaSuccess) {
            throw std::runtime_error("æµåŒæ­¥å¤±è´¥: " + std::string(cudaGetErrorString(error)));
        }

        // è½¬æ¢ä¸ºå¤æ•°æ ¼å¼
        fft_result.resize(fft_size);
        for (size_t i = 0; i < fft_size; ++i) {
            fft_result[i] = std::complex<float>(power_result[i], 0.0f);
        }

        device->processed_count++;

    } catch (const std::exception& e) {
        std::cerr << "âŒ GPU " << device->device_id << " æµ " << stream_idx << " å¤„ç†é”™è¯¯: " << e.what() << std::endl;
        device->error_count++;
        success = false;
    }

    // é‡Šæ”¾æµ
    device->stream_busy[stream_idx] = false;
    return success;
}

// åˆå§‹åŒ–å¤šGPUç³»ç»Ÿ
bool initialize_multi_gpu_system() {
    return g_gpu_manager.initialize();
}

// è·å–GPUç»Ÿè®¡ä¿¡æ¯
void print_multi_gpu_stats() {
    g_gpu_manager.printStats();
}

// è·å–å¯ç”¨GPUæ•°é‡
int get_available_gpu_count() {
    return g_gpu_manager.getAvailableGPUCount();
}

// åŸæœ‰å‡½æ•°çš„å¤šGPUç‰ˆæœ¬åŒ…è£…
bool cuda_fft_process_batch_async(const std::vector<int8_t>& chunk_data,
                                  std::vector<std::complex<float>>& fft_result) {
    return cuda_fft_process_batch_async_multigpu(chunk_data, fft_result);
}

// ä¿æŒå…¶ä»–åŸæœ‰å‡½æ•°ä¸å˜...
void cuda_fft_process_batch_streamed(const std::vector<int8_t>& chunk_data, std::vector<float>& fft_result) {
    std::vector<std::complex<float>> complex_result;
    if (cuda_fft_process_batch_async_multigpu(chunk_data, complex_result)) {
        fft_result.resize(complex_result.size());
        for (size_t i = 0; i < complex_result.size(); ++i) {
            fft_result[i] = complex_result[i].real();
        }
    }
}

void cuda_fft_process(const std::vector<int8_t>& chunk_data, std::vector<std::complex<float>>& fft_result) {
    // ä½¿ç”¨å¤šGPUç³»ç»Ÿå¤„ç†å•ä¸ªFFT
    cuda_fft_process_batch_async_multigpu(chunk_data, fft_result);
}

void cuda_fft_process_batch(const std::vector<int8_t>& chunk_data, std::vector<float>& fft_result) {
    std::vector<std::complex<float>> complex_result;
    if (cuda_fft_process_batch_async_multigpu(chunk_data, complex_result)) {
        fft_result.resize(complex_result.size());
        for (size_t i = 0; i < complex_result.size(); ++i) {
            fft_result[i] = complex_result[i].real();
        }
    }
}

void test_ConvertInt8ToFloat() {
    std::cout << "ğŸ§ª å¤šGPUç³»ç»Ÿæµ‹è¯•å®Œæˆ" << std::endl;
}